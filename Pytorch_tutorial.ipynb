{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOCM+NYO47hXMPl66Cr7Rb1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tranduytruong12/Pytorch_fundamentals/blob/main/Pytorch_tutorial.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "ZLW3RKWtgpEH"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tensor1 = torch.rand(2,)\n",
        "tensor1.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tRbKqLYvGr_0",
        "outputId": "fd4f20ec-9eee-4e4a-bb02-255f341827c4"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([2])"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Initialize tensor"
      ],
      "metadata": {
        "id": "IX_XbbBAkPhm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a 2d zeros tensor\n",
        "tensor_2d0 = torch.zeros(2,2)\n",
        "tensor_2d0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ixjca7F0ipYv",
        "outputId": "3d63bf90-afd0-40ed-cd84-cd8cc653b71b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0., 0.],\n",
              "        [0., 0.]])"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a random 2d tensor\n",
        "tensor_2d = torch.rand(2,3)\n",
        "tensor_2d"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NHyA3IJSi81R",
        "outputId": "a73a5845-30fc-4972-ee03-f35016a2d78e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.4868, 0.3586, 0.1047],\n",
              "        [0.9341, 0.4442, 0.5780]])"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Copy a numpy array to tensor\n",
        "np1 = np.array([1,2,3,4])\n",
        "tensor = torch.tensor(np1)\n",
        "tensor"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xphWFayAjDiQ",
        "outputId": "c1ae7646-458e-4ebf-9847-2482dbed50d4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([1, 2, 3, 4])"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Check data type\n",
        "tensor.dtype"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cfOj7kVujOhC",
        "outputId": "64e5f519-f4b4-4f73-97fe-a1f16db4e755"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.int64"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a tensor with range\n",
        "tensor_1_10 = torch.arange(11)\n",
        "tensor_1_10"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tD8IpVgMjQZx",
        "outputId": "2d8d905b-8da5-424f-ba37-1a83bfcfb2bc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10])"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## shaping and slicing with tensor"
      ],
      "metadata": {
        "id": "ZoLGNlhRkVRR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "my_tensor = torch.rand(2,5)"
      ],
      "metadata": {
        "id": "h1HHK6NnkFzm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "my_tensor = my_tensor.reshape(5,2)\n",
        "my_tensor"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EybBz4xGkgBu",
        "outputId": "f240ebaa-0a4e-4f33-c236-94c5890c43b3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.6956, 0.7114],\n",
              "        [0.0624, 0.4663],\n",
              "        [0.2871, 0.3475],\n",
              "        [0.1574, 0.5377],\n",
              "        [0.9361, 0.6529]])"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "my_tensor = my_tensor.reshape(10,1)\n",
        "my_tensor"
      ],
      "metadata": {
        "id": "PnN-oBLFkqMu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "my_tensor = my_tensor.reshape(1,10)"
      ],
      "metadata": {
        "id": "SLDcqxiCkv50"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "my_tensor.reshape(2,5,1)\n",
        "my_tensor"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1dxPq924kx86",
        "outputId": "36183970-7580-4de2-d117-c7b0c9a9ca96"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[0.6956],\n",
              "         [0.7114],\n",
              "         [0.0624],\n",
              "         [0.4663],\n",
              "         [0.2871]],\n",
              "\n",
              "        [[0.3475],\n",
              "         [0.1574],\n",
              "         [0.5377],\n",
              "         [0.9361],\n",
              "         [0.6529]]])"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "my_tensor.reshape(1,-1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FyjVgFKDk8j8",
        "outputId": "8dc10eb4-00ec-4d9b-c450-0a6e5e0fc32b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.6956, 0.7114, 0.0624, 0.4663, 0.2871, 0.3475, 0.1574, 0.5377, 0.9361,\n",
              "         0.6529]])"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "my_tensor.reshape(-1,2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "76dyTBrclFhP",
        "outputId": "7dbac8a3-6a3a-4ac4-bb8c-ea0e8315cd9d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.6956, 0.7114],\n",
              "        [0.0624, 0.4663],\n",
              "        [0.2871, 0.3475],\n",
              "        [0.1574, 0.5377],\n",
              "        [0.9361, 0.6529]])"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tensor_1 = torch.rand(16)"
      ],
      "metadata": {
        "id": "3sFiP49WlQ7Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tensor_2 = tensor_1.reshape(4,4)\n",
        "tensor_2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0byVEQBWl-f_",
        "outputId": "b8999ece-140a-4b1c-c94c-1aada6701ed6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.4838, 0.6710, 0.9355, 0.1566],\n",
              "        [0.3732, 0.2444, 0.9534, 0.7477],\n",
              "        [0.9768, 0.0132, 0.3452, 0.6491],\n",
              "        [0.7174, 0.1061, 0.6749, 0.9225]])"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tensor_3 = tensor_1.reshape(2,4,-1)\n",
        "tensor_3"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Hu3YO5FmJOY",
        "outputId": "aab42202-c0b1-4d50-88aa-1eb29394b1fe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[0.4838, 0.6710],\n",
              "         [0.9355, 0.1566],\n",
              "         [0.3732, 0.2444],\n",
              "         [0.9534, 0.7477]],\n",
              "\n",
              "        [[0.9768, 0.0132],\n",
              "         [0.3452, 0.6491],\n",
              "         [0.7174, 0.1061],\n",
              "         [0.6749, 0.9225]]])"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tensor_2[:,1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3TDi3b2fmQfX",
        "outputId": "69d0e79a-7ee2-49be-dbf3-e8a4fd927b66"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0.6710, 0.2444, 0.0132, 0.1061])"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tensor_2[:,1:3]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rHN9kr0nmXRP",
        "outputId": "39d0fc05-3f8d-4c72-f142-805f14631117"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.6710, 0.9355],\n",
              "        [0.2444, 0.9534],\n",
              "        [0.0132, 0.3452],\n",
              "        [0.1061, 0.6749]])"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tensor_3[:,:,-1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mxcORy-ymbNr",
        "outputId": "55a51d14-f766-4e71-9343-708bb6a54d7e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.6710, 0.1566, 0.2444, 0.7477],\n",
              "        [0.0132, 0.6491, 0.1061, 0.9225]])"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tensor_3[:,1,-1].shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-bJ_FYwRmjP4",
        "outputId": "922e79dd-82cd-4ae7-ff14-11bcd572fa3e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([2])"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Math operation"
      ],
      "metadata": {
        "id": "4kcA4nIHnAbC"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_xg7kRf2nGZN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Neural network"
      ],
      "metadata": {
        "id": "ftReDjFPv6_q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ],
      "metadata": {
        "id": "dM5_Sy7ev87C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a Model\n",
        "class Model(nn.Module):\n",
        "  def __init__(self, in_f = 4, h1 = 8, h2 = 9, out_f = 3):\n",
        "    super().__init__()\n",
        "    self.fc1 = nn.Linear(in_f, h1)\n",
        "    self.fc2 = nn.Linear(h1, h2)\n",
        "    self.out = nn.Linear(h2,out_f)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = F.relu(self.fc1(x))\n",
        "    x = F.relu(self.fc2(x))\n",
        "    x = self.out(x)\n",
        "\n",
        "    return x\n"
      ],
      "metadata": {
        "id": "Ubuoj1yNBPxr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# For consistency result\n",
        "torch.manual_seed(42)\n",
        "\n",
        "model = Model()\n"
      ],
      "metadata": {
        "id": "ATkId38mCYCy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n"
      ],
      "metadata": {
        "id": "zlEcpUgeC2qQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LNXRt3QbEXfe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "url = \"https://gist.githubusercontent.com/curran/a08a1080b88344b0c8a7/raw/0e7a9b0a5d22642a06d3d5b9bcbad9890c8ee534/iris.csv\"\n",
        "df = pd.read_csv(url)"
      ],
      "metadata": {
        "id": "sAAB4Ug7Dd29"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "Q2HQWzzxEgGE",
        "outputId": "5b7bed4b-55c8-453c-a92e-f3936f8b411b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   sepal_length  sepal_width  petal_length  petal_width species\n",
              "0           5.1          3.5           1.4          0.2  setosa\n",
              "1           4.9          3.0           1.4          0.2  setosa\n",
              "2           4.7          3.2           1.3          0.2  setosa\n",
              "3           4.6          3.1           1.5          0.2  setosa\n",
              "4           5.0          3.6           1.4          0.2  setosa"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-3562c0fd-0126-4218-bb6c-75a1bdff600a\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sepal_length</th>\n",
              "      <th>sepal_width</th>\n",
              "      <th>petal_length</th>\n",
              "      <th>petal_width</th>\n",
              "      <th>species</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>5.1</td>\n",
              "      <td>3.5</td>\n",
              "      <td>1.4</td>\n",
              "      <td>0.2</td>\n",
              "      <td>setosa</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>4.9</td>\n",
              "      <td>3.0</td>\n",
              "      <td>1.4</td>\n",
              "      <td>0.2</td>\n",
              "      <td>setosa</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>4.7</td>\n",
              "      <td>3.2</td>\n",
              "      <td>1.3</td>\n",
              "      <td>0.2</td>\n",
              "      <td>setosa</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4.6</td>\n",
              "      <td>3.1</td>\n",
              "      <td>1.5</td>\n",
              "      <td>0.2</td>\n",
              "      <td>setosa</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5.0</td>\n",
              "      <td>3.6</td>\n",
              "      <td>1.4</td>\n",
              "      <td>0.2</td>\n",
              "      <td>setosa</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-3562c0fd-0126-4218-bb6c-75a1bdff600a')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-3562c0fd-0126-4218-bb6c-75a1bdff600a button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-3562c0fd-0126-4218-bb6c-75a1bdff600a');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-b10433da-d5d7-49e2-a7d5-4317d78de258\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-b10433da-d5d7-49e2-a7d5-4317d78de258')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-b10433da-d5d7-49e2-a7d5-4317d78de258 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 150,\n  \"fields\": [\n    {\n      \"column\": \"sepal_length\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.8280661279778629,\n        \"min\": 4.3,\n        \"max\": 7.9,\n        \"num_unique_values\": 35,\n        \"samples\": [\n          6.2,\n          4.5,\n          5.6\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"sepal_width\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.4335943113621737,\n        \"min\": 2.0,\n        \"max\": 4.4,\n        \"num_unique_values\": 23,\n        \"samples\": [\n          2.3,\n          4.0,\n          3.5\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"petal_length\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1.7644204199522617,\n        \"min\": 1.0,\n        \"max\": 6.9,\n        \"num_unique_values\": 43,\n        \"samples\": [\n          6.7,\n          3.8,\n          3.7\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"petal_width\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.7631607417008414,\n        \"min\": 0.1,\n        \"max\": 2.5,\n        \"num_unique_values\": 22,\n        \"samples\": [\n          0.2,\n          1.2,\n          1.3\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"species\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          \"setosa\",\n          \"versicolor\",\n          \"virginica\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X = df.drop('species', axis = 1)\n",
        "y = df['species']"
      ],
      "metadata": {
        "id": "K9xj095_EhpY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y = y.replace('setosa', 0)\n",
        "y = y.replace('virginica', 1)\n",
        "y = y.replace('versicolor', 2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JyDyHzUbNSCn",
        "outputId": "46e84175-1917-4aea-dda9-25c2d2dca498"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-29-f45f0c9f0dad>:3: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
            "  y = y.replace('versicolor', 2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X = X.values\n",
        "y = y.values"
      ],
      "metadata": {
        "id": "Q_X9W_96Ev4R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.2, random_state = 41)\n",
        "X_train = torch.FloatTensor(X_train)\n",
        "X_test = torch.FloatTensor(X_test)\n",
        "y_train = torch.LongTensor(y_train)\n",
        "y_test = torch.LongTensor(y_test)"
      ],
      "metadata": {
        "id": "OvVIOiouMsWw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_train"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NhzPM9ktNLtI",
        "outputId": "3f544631-933c-434d-de01-17a696fe78d1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([2, 2, 1, 2, 1, 0, 1, 2, 1, 2, 2, 2, 0, 1, 1, 0, 1, 2, 0, 2, 1, 2, 1, 0,\n",
              "        0, 1, 1, 0, 2, 0, 0, 2, 0, 2, 1, 0, 2, 2, 1, 0, 2, 2, 0, 1, 2, 0, 1, 1,\n",
              "        0, 2, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 2, 0, 0, 1, 1, 1, 2, 2,\n",
              "        1, 0, 2, 1, 2, 2, 0, 2, 0, 2, 0, 2, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 2, 1,\n",
              "        0, 1, 0, 2, 1, 2, 0, 1, 0, 2, 0, 1, 2, 2, 1, 2, 2, 1, 1, 0, 2, 2, 2, 1])"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = Model()\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr = 0.001)"
      ],
      "metadata": {
        "id": "pCZorSHdNyo2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.parameters"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "id": "aYbqbWqtOy29",
        "outputId": "f808df6c-d502-4736-d7e1-4b0073d112e4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<bound method Module.parameters of Model(\n",
              "  (fc1): Linear(in_features=4, out_features=8, bias=True)\n",
              "  (fc2): Linear(in_features=8, out_features=9, bias=True)\n",
              "  (out): Linear(in_features=9, out_features=3, bias=True)\n",
              ")>"
            ],
            "text/html": [
              "<div style=\"max-width:800px; border: 1px solid var(--colab-border-color);\"><style>\n",
              "      pre.function-repr-contents {\n",
              "        overflow-x: auto;\n",
              "        padding: 8px 12px;\n",
              "        max-height: 500px;\n",
              "      }\n",
              "\n",
              "      pre.function-repr-contents.function-repr-contents-collapsed {\n",
              "        cursor: pointer;\n",
              "        max-height: 100px;\n",
              "      }\n",
              "    </style>\n",
              "    <pre style=\"white-space: initial; background:\n",
              "         var(--colab-secondary-surface-color); padding: 8px 12px;\n",
              "         border-bottom: 1px solid var(--colab-border-color);\"><b>torch.nn.modules.module.Module.parameters</b><br/>def parameters(recurse: bool=True) -&gt; Iterator[Parameter]</pre><pre class=\"function-repr-contents function-repr-contents-collapsed\" style=\"\"><a class=\"filepath\" style=\"display:none\" href=\"#\">/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py</a>Return an iterator over module parameters.\n",
              "\n",
              "This is typically passed to an optimizer.\n",
              "\n",
              "Args:\n",
              "    recurse (bool): if True, then yields parameters of this module\n",
              "        and all submodules. Otherwise, yields only parameters that\n",
              "        are direct members of this module.\n",
              "\n",
              "Yields:\n",
              "    Parameter: module parameter\n",
              "\n",
              "Example::\n",
              "\n",
              "    &gt;&gt;&gt; # xdoctest: +SKIP(&quot;undefined vars&quot;)\n",
              "    &gt;&gt;&gt; for param in model.parameters():\n",
              "    &gt;&gt;&gt;     print(type(param), param.size())\n",
              "    &lt;class &#x27;torch.Tensor&#x27;&gt; (20L,)\n",
              "    &lt;class &#x27;torch.Tensor&#x27;&gt; (20L, 1L, 5L, 5L)</pre>\n",
              "      <script>\n",
              "      if (google.colab.kernel.accessAllowed && google.colab.files && google.colab.files.view) {\n",
              "        for (const element of document.querySelectorAll('.filepath')) {\n",
              "          element.style.display = 'block'\n",
              "          element.onclick = (event) => {\n",
              "            event.preventDefault();\n",
              "            event.stopPropagation();\n",
              "            google.colab.files.view(element.textContent, 2608);\n",
              "          };\n",
              "        }\n",
              "      }\n",
              "      for (const element of document.querySelectorAll('.function-repr-contents')) {\n",
              "        element.onclick = (event) => {\n",
              "          event.preventDefault();\n",
              "          event.stopPropagation();\n",
              "          element.classList.toggle('function-repr-contents-collapsed');\n",
              "        };\n",
              "      }\n",
              "      </script>\n",
              "      </div>"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "epochs =  10000\n",
        "losses = []\n",
        "for i in range(epochs):\n",
        "  y_pred = model.forward(X_train)\n",
        "  # Measure loss\n",
        "  loss = criterion(y_pred, y_train)\n",
        "\n",
        "  losses.append(loss.detach().numpy())\n",
        "\n",
        "  if i % 10 == 0:\n",
        "    print(f'Epoch {i}: loss is {loss}.')\n",
        "\n",
        "  # Back prob\n",
        "  optimizer.zero_grad()\n",
        "  loss.backward()\n",
        "  optimizer.step()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6PVq2O0HSeHA",
        "outputId": "0ea29b90-c433-4f19-899b-9dbc842435a2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0: loss is 0.04529167711734772.\n",
            "Epoch 10: loss is 0.044632453471422195.\n",
            "Epoch 20: loss is 0.04399566724896431.\n",
            "Epoch 30: loss is 0.0433804988861084.\n",
            "Epoch 40: loss is 0.04278559982776642.\n",
            "Epoch 50: loss is 0.04220999404788017.\n",
            "Epoch 60: loss is 0.04165264591574669.\n",
            "Epoch 70: loss is 0.04111260548233986.\n",
            "Epoch 80: loss is 0.040589071810245514.\n",
            "Epoch 90: loss is 0.04008125141263008.\n",
            "Epoch 100: loss is 0.03958837687969208.\n",
            "Epoch 110: loss is 0.03910980001091957.\n",
            "Epoch 120: loss is 0.03864491730928421.\n",
            "Epoch 130: loss is 0.038193073123693466.\n",
            "Epoch 140: loss is 0.03775374963879585.\n",
            "Epoch 150: loss is 0.0373263917863369.\n",
            "Epoch 160: loss is 0.036911290138959885.\n",
            "Epoch 170: loss is 0.03650782257318497.\n",
            "Epoch 180: loss is 0.0361153706908226.\n",
            "Epoch 190: loss is 0.0357331857085228.\n",
            "Epoch 200: loss is 0.03536093607544899.\n",
            "Epoch 210: loss is 0.03499802574515343.\n",
            "Epoch 220: loss is 0.034644100815057755.\n",
            "Epoch 230: loss is 0.034298766404390335.\n",
            "Epoch 240: loss is 0.0339617095887661.\n",
            "Epoch 250: loss is 0.033632680773735046.\n",
            "Epoch 260: loss is 0.03331137076020241.\n",
            "Epoch 270: loss is 0.032997895032167435.\n",
            "Epoch 280: loss is 0.03269194811582565.\n",
            "Epoch 290: loss is 0.03239293769001961.\n",
            "Epoch 300: loss is 0.03210059925913811.\n",
            "Epoch 310: loss is 0.03181467950344086.\n",
            "Epoch 320: loss is 0.031534891575574875.\n",
            "Epoch 330: loss is 0.03126106411218643.\n",
            "Epoch 340: loss is 0.030992982909083366.\n",
            "Epoch 350: loss is 0.03073040395975113.\n",
            "Epoch 360: loss is 0.030473215505480766.\n",
            "Epoch 370: loss is 0.03022116981446743.\n",
            "Epoch 380: loss is 0.02997446432709694.\n",
            "Epoch 390: loss is 0.02973300591111183.\n",
            "Epoch 400: loss is 0.029496291652321815.\n",
            "Epoch 410: loss is 0.029264116659760475.\n",
            "Epoch 420: loss is 0.02903631515800953.\n",
            "Epoch 430: loss is 0.028812777251005173.\n",
            "Epoch 440: loss is 0.028593292459845543.\n",
            "Epoch 450: loss is 0.028377825394272804.\n",
            "Epoch 460: loss is 0.028166158124804497.\n",
            "Epoch 470: loss is 0.02795824222266674.\n",
            "Epoch 480: loss is 0.02775394730269909.\n",
            "Epoch 490: loss is 0.027553286403417587.\n",
            "Epoch 500: loss is 0.027356091886758804.\n",
            "Epoch 510: loss is 0.027162250131368637.\n",
            "Epoch 520: loss is 0.026971634477376938.\n",
            "Epoch 530: loss is 0.026784120127558708.\n",
            "Epoch 540: loss is 0.026599634438753128.\n",
            "Epoch 550: loss is 0.026418093591928482.\n",
            "Epoch 560: loss is 0.02623937651515007.\n",
            "Epoch 570: loss is 0.026063449680805206.\n",
            "Epoch 580: loss is 0.025890206918120384.\n",
            "Epoch 590: loss is 0.025719614699482918.\n",
            "Epoch 600: loss is 0.02555154822766781.\n",
            "Epoch 610: loss is 0.025385979562997818.\n",
            "Epoch 620: loss is 0.02522283047437668.\n",
            "Epoch 630: loss is 0.02506203018128872.\n",
            "Epoch 640: loss is 0.024903515353798866.\n",
            "Epoch 650: loss is 0.024747278541326523.\n",
            "Epoch 660: loss is 0.024593425914645195.\n",
            "Epoch 670: loss is 0.024441756308078766.\n",
            "Epoch 680: loss is 0.024292171001434326.\n",
            "Epoch 690: loss is 0.02414456009864807.\n",
            "Epoch 700: loss is 0.023998906835913658.\n",
            "Epoch 710: loss is 0.02385530062019825.\n",
            "Epoch 720: loss is 0.02371371164917946.\n",
            "Epoch 730: loss is 0.023573987185955048.\n",
            "Epoch 740: loss is 0.02343602664768696.\n",
            "Epoch 750: loss is 0.023299753665924072.\n",
            "Epoch 760: loss is 0.023165173828601837.\n",
            "Epoch 770: loss is 0.023032154887914658.\n",
            "Epoch 780: loss is 0.022900719195604324.\n",
            "Epoch 790: loss is 0.02277081087231636.\n",
            "Epoch 800: loss is 0.02264244668185711.\n",
            "Epoch 810: loss is 0.02251552604138851.\n",
            "Epoch 820: loss is 0.02239006571471691.\n",
            "Epoch 830: loss is 0.022266002371907234.\n",
            "Epoch 840: loss is 0.02214326336979866.\n",
            "Epoch 850: loss is 0.02202184870839119.\n",
            "Epoch 860: loss is 0.021901732310652733.\n",
            "Epoch 870: loss is 0.021782953292131424.\n",
            "Epoch 880: loss is 0.02166544459760189.\n",
            "Epoch 890: loss is 0.021549129858613014.\n",
            "Epoch 900: loss is 0.02143406867980957.\n",
            "Epoch 910: loss is 0.02132052183151245.\n",
            "Epoch 920: loss is 0.021208113059401512.\n",
            "Epoch 930: loss is 0.0210968479514122.\n",
            "Epoch 940: loss is 0.020986590534448624.\n",
            "Epoch 950: loss is 0.02087738737463951.\n",
            "Epoch 960: loss is 0.020769214257597923.\n",
            "Epoch 970: loss is 0.020662035793066025.\n",
            "Epoch 980: loss is 0.020555807277560234.\n",
            "Epoch 990: loss is 0.020450567826628685.\n",
            "Epoch 1000: loss is 0.02034619450569153.\n",
            "Epoch 1010: loss is 0.02024274319410324.\n",
            "Epoch 1020: loss is 0.020140130072832108.\n",
            "Epoch 1030: loss is 0.02003839798271656.\n",
            "Epoch 1040: loss is 0.019937606528401375.\n",
            "Epoch 1050: loss is 0.019837798550724983.\n",
            "Epoch 1060: loss is 0.019738780334591866.\n",
            "Epoch 1070: loss is 0.019640499725937843.\n",
            "Epoch 1080: loss is 0.019543064758181572.\n",
            "Epoch 1090: loss is 0.01944638229906559.\n",
            "Epoch 1100: loss is 0.019350389018654823.\n",
            "Epoch 1110: loss is 0.019255058839917183.\n",
            "Epoch 1120: loss is 0.019160402938723564.\n",
            "Epoch 1130: loss is 0.019066419452428818.\n",
            "Epoch 1140: loss is 0.018973125144839287.\n",
            "Epoch 1150: loss is 0.01888047717511654.\n",
            "Epoch 1160: loss is 0.018788456916809082.\n",
            "Epoch 1170: loss is 0.018697010353207588.\n",
            "Epoch 1180: loss is 0.01860618405044079.\n",
            "Epoch 1190: loss is 0.018515966832637787.\n",
            "Epoch 1200: loss is 0.018426286056637764.\n",
            "Epoch 1210: loss is 0.018337180837988853.\n",
            "Epoch 1220: loss is 0.018248597159981728.\n",
            "Epoch 1230: loss is 0.018160531297326088.\n",
            "Epoch 1240: loss is 0.018072960898280144.\n",
            "Epoch 1250: loss is 0.017985893413424492.\n",
            "Epoch 1260: loss is 0.017899315804243088.\n",
            "Epoch 1270: loss is 0.01781320758163929.\n",
            "Epoch 1280: loss is 0.01772756315767765.\n",
            "Epoch 1290: loss is 0.017642367631196976.\n",
            "Epoch 1300: loss is 0.017557600513100624.\n",
            "Epoch 1310: loss is 0.01747322641313076.\n",
            "Epoch 1320: loss is 0.017389290034770966.\n",
            "Epoch 1330: loss is 0.017305796965956688.\n",
            "Epoch 1340: loss is 0.017222678288817406.\n",
            "Epoch 1350: loss is 0.01713992841541767.\n",
            "Epoch 1360: loss is 0.017057614400982857.\n",
            "Epoch 1370: loss is 0.016975633800029755.\n",
            "Epoch 1380: loss is 0.016894137486815453.\n",
            "Epoch 1390: loss is 0.016813106834888458.\n",
            "Epoch 1400: loss is 0.016732383519411087.\n",
            "Epoch 1410: loss is 0.016652056947350502.\n",
            "Epoch 1420: loss is 0.016572047024965286.\n",
            "Epoch 1430: loss is 0.016492420807480812.\n",
            "Epoch 1440: loss is 0.016413085162639618.\n",
            "Epoch 1450: loss is 0.016334090381860733.\n",
            "Epoch 1460: loss is 0.016255419701337814.\n",
            "Epoch 1470: loss is 0.016177039593458176.\n",
            "Epoch 1480: loss is 0.016098953783512115.\n",
            "Epoch 1490: loss is 0.01602114364504814.\n",
            "Epoch 1500: loss is 0.015943653881549835.\n",
            "Epoch 1510: loss is 0.015866413712501526.\n",
            "Epoch 1520: loss is 0.015789426863193512.\n",
            "Epoch 1530: loss is 0.01571274735033512.\n",
            "Epoch 1540: loss is 0.01563628390431404.\n",
            "Epoch 1550: loss is 0.015560130588710308.\n",
            "Epoch 1560: loss is 0.015484195202589035.\n",
            "Epoch 1570: loss is 0.015408528968691826.\n",
            "Epoch 1580: loss is 0.01533308532088995.\n",
            "Epoch 1590: loss is 0.015257881954312325.\n",
            "Epoch 1600: loss is 0.015182916074991226.\n",
            "Epoch 1610: loss is 0.015108179301023483.\n",
            "Epoch 1620: loss is 0.015033653937280178.\n",
            "Epoch 1630: loss is 0.014959398657083511.\n",
            "Epoch 1640: loss is 0.01488531194627285.\n",
            "Epoch 1650: loss is 0.014811443164944649.\n",
            "Epoch 1660: loss is 0.01473778486251831.\n",
            "Epoch 1670: loss is 0.01466433610767126.\n",
            "Epoch 1680: loss is 0.014591128565371037.\n",
            "Epoch 1690: loss is 0.01451809797435999.\n",
            "Epoch 1700: loss is 0.01444527693092823.\n",
            "Epoch 1710: loss is 0.01437262911349535.\n",
            "Epoch 1720: loss is 0.014300205744802952.\n",
            "Epoch 1730: loss is 0.01422796305269003.\n",
            "Epoch 1740: loss is 0.014155900105834007.\n",
            "Epoch 1750: loss is 0.014084028080105782.\n",
            "Epoch 1760: loss is 0.01401233859360218.\n",
            "Epoch 1770: loss is 0.013940960168838501.\n",
            "Epoch 1780: loss is 0.013869987800717354.\n",
            "Epoch 1790: loss is 0.013799218460917473.\n",
            "Epoch 1800: loss is 0.013728641904890537.\n",
            "Epoch 1810: loss is 0.013658221811056137.\n",
            "Epoch 1820: loss is 0.013587933965027332.\n",
            "Epoch 1830: loss is 0.013517885468900204.\n",
            "Epoch 1840: loss is 0.01344794686883688.\n",
            "Epoch 1850: loss is 0.01337813027203083.\n",
            "Epoch 1860: loss is 0.01330853346735239.\n",
            "Epoch 1870: loss is 0.013239072635769844.\n",
            "Epoch 1880: loss is 0.013169731944799423.\n",
            "Epoch 1890: loss is 0.01310058869421482.\n",
            "Epoch 1900: loss is 0.013031557202339172.\n",
            "Epoch 1910: loss is 0.012962727807462215.\n",
            "Epoch 1920: loss is 0.012894018553197384.\n",
            "Epoch 1930: loss is 0.012825465761125088.\n",
            "Epoch 1940: loss is 0.01275707595050335.\n",
            "Epoch 1950: loss is 0.01268879696726799.\n",
            "Epoch 1960: loss is 0.012620702385902405.\n",
            "Epoch 1970: loss is 0.012552754953503609.\n",
            "Epoch 1980: loss is 0.012484941631555557.\n",
            "Epoch 1990: loss is 0.012417281046509743.\n",
            "Epoch 2000: loss is 0.012349803932011127.\n",
            "Epoch 2010: loss is 0.01228246372193098.\n",
            "Epoch 2020: loss is 0.012215305119752884.\n",
            "Epoch 2030: loss is 0.012148339301347733.\n",
            "Epoch 2040: loss is 0.012081483379006386.\n",
            "Epoch 2050: loss is 0.01201481930911541.\n",
            "Epoch 2060: loss is 0.011948301456868649.\n",
            "Epoch 2070: loss is 0.0118819335475564.\n",
            "Epoch 2080: loss is 0.011815967969596386.\n",
            "Epoch 2090: loss is 0.011750364676117897.\n",
            "Epoch 2100: loss is 0.011684988625347614.\n",
            "Epoch 2110: loss is 0.011619710363447666.\n",
            "Epoch 2120: loss is 0.011554635129868984.\n",
            "Epoch 2130: loss is 0.011489680036902428.\n",
            "Epoch 2140: loss is 0.011424863710999489.\n",
            "Epoch 2150: loss is 0.011360188946127892.\n",
            "Epoch 2160: loss is 0.011295667849481106.\n",
            "Epoch 2170: loss is 0.011231261305510998.\n",
            "Epoch 2180: loss is 0.011167040094733238.\n",
            "Epoch 2190: loss is 0.011102926917374134.\n",
            "Epoch 2200: loss is 0.011039008386433125.\n",
            "Epoch 2210: loss is 0.010975213721394539.\n",
            "Epoch 2220: loss is 0.010911572724580765.\n",
            "Epoch 2230: loss is 0.010848105885088444.\n",
            "Epoch 2240: loss is 0.010784736834466457.\n",
            "Epoch 2250: loss is 0.010721593163907528.\n",
            "Epoch 2260: loss is 0.01065854076296091.\n",
            "Epoch 2270: loss is 0.010595685802400112.\n",
            "Epoch 2280: loss is 0.010533015243709087.\n",
            "Epoch 2290: loss is 0.010470492765307426.\n",
            "Epoch 2300: loss is 0.010408111847937107.\n",
            "Epoch 2310: loss is 0.010345911607146263.\n",
            "Epoch 2320: loss is 0.01028388924896717.\n",
            "Epoch 2330: loss is 0.010222041048109531.\n",
            "Epoch 2340: loss is 0.010160354897379875.\n",
            "Epoch 2350: loss is 0.010098828934133053.\n",
            "Epoch 2360: loss is 0.010037541389465332.\n",
            "Epoch 2370: loss is 0.009976476430892944.\n",
            "Epoch 2380: loss is 0.009915555827319622.\n",
            "Epoch 2390: loss is 0.009854835458099842.\n",
            "Epoch 2400: loss is 0.009794270619750023.\n",
            "Epoch 2410: loss is 0.009733890183269978.\n",
            "Epoch 2420: loss is 0.009673713706433773.\n",
            "Epoch 2430: loss is 0.009613734669983387.\n",
            "Epoch 2440: loss is 0.009553924202919006.\n",
            "Epoch 2450: loss is 0.009494300000369549.\n",
            "Epoch 2460: loss is 0.009434898383915424.\n",
            "Epoch 2470: loss is 0.00937564205378294.\n",
            "Epoch 2480: loss is 0.009316626004874706.\n",
            "Epoch 2490: loss is 0.009257761761546135.\n",
            "Epoch 2500: loss is 0.009199123829603195.\n",
            "Epoch 2510: loss is 0.009140667505562305.\n",
            "Epoch 2520: loss is 0.009082433767616749.\n",
            "Epoch 2530: loss is 0.009024376980960369.\n",
            "Epoch 2540: loss is 0.008966535329818726.\n",
            "Epoch 2550: loss is 0.008908906951546669.\n",
            "Epoch 2560: loss is 0.008851462975144386.\n",
            "Epoch 2570: loss is 0.008794236928224564.\n",
            "Epoch 2580: loss is 0.008737231604754925.\n",
            "Epoch 2590: loss is 0.008680429309606552.\n",
            "Epoch 2600: loss is 0.008623838424682617.\n",
            "Epoch 2610: loss is 0.00856746081262827.\n",
            "Epoch 2620: loss is 0.008511313237249851.\n",
            "Epoch 2630: loss is 0.008455363102257252.\n",
            "Epoch 2640: loss is 0.00839963834732771.\n",
            "Epoch 2650: loss is 0.008344113826751709.\n",
            "Epoch 2660: loss is 0.008288828656077385.\n",
            "Epoch 2670: loss is 0.008233805187046528.\n",
            "Epoch 2680: loss is 0.008178948424756527.\n",
            "Epoch 2690: loss is 0.008124353364109993.\n",
            "Epoch 2700: loss is 0.00806995015591383.\n",
            "Epoch 2710: loss is 0.008015783503651619.\n",
            "Epoch 2720: loss is 0.007961860857903957.\n",
            "Epoch 2730: loss is 0.007908185012638569.\n",
            "Epoch 2740: loss is 0.007854715920984745.\n",
            "Epoch 2750: loss is 0.00780148571357131.\n",
            "Epoch 2760: loss is 0.007747826632112265.\n",
            "Epoch 2770: loss is 0.007694152649492025.\n",
            "Epoch 2780: loss is 0.007640652358531952.\n",
            "Epoch 2790: loss is 0.007587366737425327.\n",
            "Epoch 2800: loss is 0.007534367963671684.\n",
            "Epoch 2810: loss is 0.007481591776013374.\n",
            "Epoch 2820: loss is 0.007429101038724184.\n",
            "Epoch 2830: loss is 0.0073768822476267815.\n",
            "Epoch 2840: loss is 0.007324923761188984.\n",
            "Epoch 2850: loss is 0.007273274008184671.\n",
            "Epoch 2860: loss is 0.007222078274935484.\n",
            "Epoch 2870: loss is 0.007170814089477062.\n",
            "Epoch 2880: loss is 0.0071198479272425175.\n",
            "Epoch 2890: loss is 0.00706919701769948.\n",
            "Epoch 2900: loss is 0.007018858566880226.\n",
            "Epoch 2910: loss is 0.006968767382204533.\n",
            "Epoch 2920: loss is 0.006918951403349638.\n",
            "Epoch 2930: loss is 0.0068694050423800945.\n",
            "Epoch 2940: loss is 0.006820125505328178.\n",
            "Epoch 2950: loss is 0.00677111092954874.\n",
            "Epoch 2960: loss is 0.006722377147525549.\n",
            "Epoch 2970: loss is 0.006673917640000582.\n",
            "Epoch 2980: loss is 0.006625659763813019.\n",
            "Epoch 2990: loss is 0.006577714812010527.\n",
            "Epoch 3000: loss is 0.006530028767883778.\n",
            "Epoch 3010: loss is 0.006482591386884451.\n",
            "Epoch 3020: loss is 0.00643542967736721.\n",
            "Epoch 3030: loss is 0.006388567853718996.\n",
            "Epoch 3040: loss is 0.006341987289488316.\n",
            "Epoch 3050: loss is 0.006295556668192148.\n",
            "Epoch 3060: loss is 0.006248755846172571.\n",
            "Epoch 3070: loss is 0.0062023368664085865.\n",
            "Epoch 3080: loss is 0.00615621916949749.\n",
            "Epoch 3090: loss is 0.006110362242907286.\n",
            "Epoch 3100: loss is 0.006064769346266985.\n",
            "Epoch 3110: loss is 0.006019460037350655.\n",
            "Epoch 3120: loss is 0.005974413827061653.\n",
            "Epoch 3130: loss is 0.005929647479206324.\n",
            "Epoch 3140: loss is 0.005885118152946234.\n",
            "Epoch 3150: loss is 0.00584088359028101.\n",
            "Epoch 3160: loss is 0.005796925164759159.\n",
            "Epoch 3170: loss is 0.005753213074058294.\n",
            "Epoch 3180: loss is 0.005709797143936157.\n",
            "Epoch 3190: loss is 0.005667181685566902.\n",
            "Epoch 3200: loss is 0.005623818840831518.\n",
            "Epoch 3210: loss is 0.005581074859946966.\n",
            "Epoch 3220: loss is 0.005538719240576029.\n",
            "Epoch 3230: loss is 0.005496602971106768.\n",
            "Epoch 3240: loss is 0.005454742815345526.\n",
            "Epoch 3250: loss is 0.005413202568888664.\n",
            "Epoch 3260: loss is 0.005371863022446632.\n",
            "Epoch 3270: loss is 0.0053307730704545975.\n",
            "Epoch 3280: loss is 0.005289989057928324.\n",
            "Epoch 3290: loss is 0.005249428562819958.\n",
            "Epoch 3300: loss is 0.0052091279067099094.\n",
            "Epoch 3310: loss is 0.00516910944133997.\n",
            "Epoch 3320: loss is 0.0051293279975652695.\n",
            "Epoch 3330: loss is 0.005089785438030958.\n",
            "Epoch 3340: loss is 0.005050518084317446.\n",
            "Epoch 3350: loss is 0.005011497065424919.\n",
            "Epoch 3360: loss is 0.004972995258867741.\n",
            "Epoch 3370: loss is 0.004934222437441349.\n",
            "Epoch 3380: loss is 0.0048959990963339806.\n",
            "Epoch 3390: loss is 0.004857967142015696.\n",
            "Epoch 3400: loss is 0.0048201847821474075.\n",
            "Epoch 3410: loss is 0.004782678559422493.\n",
            "Epoch 3420: loss is 0.004745417274534702.\n",
            "Epoch 3430: loss is 0.0047083888202905655.\n",
            "Epoch 3440: loss is 0.004671606235206127.\n",
            "Epoch 3450: loss is 0.0046350667253136635.\n",
            "Epoch 3460: loss is 0.004598778206855059.\n",
            "Epoch 3470: loss is 0.004562726244330406.\n",
            "Epoch 3480: loss is 0.004526892211288214.\n",
            "Epoch 3490: loss is 0.004491342231631279.\n",
            "Epoch 3500: loss is 0.004456000402569771.\n",
            "Epoch 3510: loss is 0.004421050660312176.\n",
            "Epoch 3520: loss is 0.004386121407151222.\n",
            "Epoch 3530: loss is 0.0043514263816177845.\n",
            "Epoch 3540: loss is 0.004317044746130705.\n",
            "Epoch 3550: loss is 0.004282895941287279.\n",
            "Epoch 3560: loss is 0.00424896739423275.\n",
            "Epoch 3570: loss is 0.004215280991047621.\n",
            "Epoch 3580: loss is 0.004181822296231985.\n",
            "Epoch 3590: loss is 0.004148602951318026.\n",
            "Epoch 3600: loss is 0.004115593619644642.\n",
            "Epoch 3610: loss is 0.004082845523953438.\n",
            "Epoch 3620: loss is 0.004050284158438444.\n",
            "Epoch 3630: loss is 0.004017970059067011.\n",
            "Epoch 3640: loss is 0.0039884052239358425.\n",
            "Epoch 3650: loss is 0.00395840173587203.\n",
            "Epoch 3660: loss is 0.0039240154437720776.\n",
            "Epoch 3670: loss is 0.003892125329002738.\n",
            "Epoch 3680: loss is 0.0038612305652350187.\n",
            "Epoch 3690: loss is 0.003830409375950694.\n",
            "Epoch 3700: loss is 0.003800036618486047.\n",
            "Epoch 3710: loss is 0.003769675735384226.\n",
            "Epoch 3720: loss is 0.0037396675907075405.\n",
            "Epoch 3730: loss is 0.0037098610773682594.\n",
            "Epoch 3740: loss is 0.0036802445538342.\n",
            "Epoch 3750: loss is 0.003650867147371173.\n",
            "Epoch 3760: loss is 0.003621704876422882.\n",
            "Epoch 3770: loss is 0.0035927719436585903.\n",
            "Epoch 3780: loss is 0.003564020385965705.\n",
            "Epoch 3790: loss is 0.0035354583524167538.\n",
            "Epoch 3800: loss is 0.003507177112624049.\n",
            "Epoch 3810: loss is 0.0034790383651852608.\n",
            "Epoch 3820: loss is 0.003451048396527767.\n",
            "Epoch 3830: loss is 0.0034232877660542727.\n",
            "Epoch 3840: loss is 0.0033960766158998013.\n",
            "Epoch 3850: loss is 0.0033683902584016323.\n",
            "Epoch 3860: loss is 0.0033413211349397898.\n",
            "Epoch 3870: loss is 0.0033143050968647003.\n",
            "Epoch 3880: loss is 0.0032876133918762207.\n",
            "Epoch 3890: loss is 0.003261399455368519.\n",
            "Epoch 3900: loss is 0.00323481229133904.\n",
            "Epoch 3910: loss is 0.0032086933497339487.\n",
            "Epoch 3920: loss is 0.0031828591600060463.\n",
            "Epoch 3930: loss is 0.003157194470986724.\n",
            "Epoch 3940: loss is 0.0031316159293055534.\n",
            "Epoch 3950: loss is 0.0031062313355505466.\n",
            "Epoch 3960: loss is 0.00308134569786489.\n",
            "Epoch 3970: loss is 0.0030562037136405706.\n",
            "Epoch 3980: loss is 0.0030326477717608213.\n",
            "Epoch 3990: loss is 0.003007116261869669.\n",
            "Epoch 4000: loss is 0.0029826383106410503.\n",
            "Epoch 4010: loss is 0.002958243479952216.\n",
            "Epoch 4020: loss is 0.0029342169873416424.\n",
            "Epoch 4030: loss is 0.002910291077569127.\n",
            "Epoch 4040: loss is 0.002886601025238633.\n",
            "Epoch 4050: loss is 0.0028631207533180714.\n",
            "Epoch 4060: loss is 0.002839796245098114.\n",
            "Epoch 4070: loss is 0.0028166777919977903.\n",
            "Epoch 4080: loss is 0.0027936978731304407.\n",
            "Epoch 4090: loss is 0.0027709603309631348.\n",
            "Epoch 4100: loss is 0.0027482586447149515.\n",
            "Epoch 4110: loss is 0.002725806552916765.\n",
            "Epoch 4120: loss is 0.0027034960221499205.\n",
            "Epoch 4130: loss is 0.0026813724543899298.\n",
            "Epoch 4140: loss is 0.002660013036802411.\n",
            "Epoch 4150: loss is 0.0026376901660114527.\n",
            "Epoch 4160: loss is 0.0026161898858845234.\n",
            "Epoch 4170: loss is 0.0025946248788386583.\n",
            "Epoch 4180: loss is 0.002573265926912427.\n",
            "Epoch 4190: loss is 0.00255209906026721.\n",
            "Epoch 4200: loss is 0.0025311079807579517.\n",
            "Epoch 4210: loss is 0.0025103248190134764.\n",
            "Epoch 4220: loss is 0.0024899840354919434.\n",
            "Epoch 4230: loss is 0.002469355007633567.\n",
            "Epoch 4240: loss is 0.0024488605558872223.\n",
            "Epoch 4250: loss is 0.002428612904623151.\n",
            "Epoch 4260: loss is 0.002408553846180439.\n",
            "Epoch 4270: loss is 0.0023886780254542828.\n",
            "Epoch 4280: loss is 0.0023689507506787777.\n",
            "Epoch 4290: loss is 0.002349372487515211.\n",
            "Epoch 4300: loss is 0.0023302400950342417.\n",
            "Epoch 4310: loss is 0.0023106771986931562.\n",
            "Epoch 4320: loss is 0.002291555516421795.\n",
            "Epoch 4330: loss is 0.0022725712042301893.\n",
            "Epoch 4340: loss is 0.002253746148198843.\n",
            "Epoch 4350: loss is 0.002235045423731208.\n",
            "Epoch 4360: loss is 0.002216504653915763.\n",
            "Epoch 4370: loss is 0.002198103815317154.\n",
            "Epoch 4380: loss is 0.0021798412781208754.\n",
            "Epoch 4390: loss is 0.0021617284510284662.\n",
            "Epoch 4400: loss is 0.0021437411196529865.\n",
            "Epoch 4410: loss is 0.0021258951164782047.\n",
            "Epoch 4420: loss is 0.0021082053426653147.\n",
            "Epoch 4430: loss is 0.002090650610625744.\n",
            "Epoch 4440: loss is 0.002073227195069194.\n",
            "Epoch 4450: loss is 0.002055930206552148.\n",
            "Epoch 4460: loss is 0.0020387833938002586.\n",
            "Epoch 4470: loss is 0.0020217527635395527.\n",
            "Epoch 4480: loss is 0.002004856476560235.\n",
            "Epoch 4490: loss is 0.0019881154876202345.\n",
            "Epoch 4500: loss is 0.001971487421542406.\n",
            "Epoch 4510: loss is 0.0019549934659153223.\n",
            "Epoch 4520: loss is 0.00193862768355757.\n",
            "Epoch 4530: loss is 0.0019223979907110333.\n",
            "Epoch 4540: loss is 0.0019062749342992902.\n",
            "Epoch 4550: loss is 0.0018903003074228764.\n",
            "Epoch 4560: loss is 0.0018744453554973006.\n",
            "Epoch 4570: loss is 0.0018587211379781365.\n",
            "Epoch 4580: loss is 0.0018431226490065455.\n",
            "Epoch 4590: loss is 0.0018276547780260444.\n",
            "Epoch 4600: loss is 0.001812295289710164.\n",
            "Epoch 4610: loss is 0.0017970597837120295.\n",
            "Epoch 4620: loss is 0.001781946630217135.\n",
            "Epoch 4630: loss is 0.0017669578082859516.\n",
            "Epoch 4640: loss is 0.00175209972076118.\n",
            "Epoch 4650: loss is 0.0017373402370139956.\n",
            "Epoch 4660: loss is 0.0017227099742740393.\n",
            "Epoch 4670: loss is 0.0017081939149647951.\n",
            "Epoch 4680: loss is 0.0016937976470217109.\n",
            "Epoch 4690: loss is 0.0016795082483440638.\n",
            "Epoch 4700: loss is 0.0016653520287945867.\n",
            "Epoch 4710: loss is 0.0016513121081516147.\n",
            "Epoch 4720: loss is 0.0016373629914596677.\n",
            "Epoch 4730: loss is 0.0016235524090006948.\n",
            "Epoch 4740: loss is 0.0016098299529403448.\n",
            "Epoch 4750: loss is 0.0015962353209033608.\n",
            "Epoch 4760: loss is 0.0015827456954866648.\n",
            "Epoch 4770: loss is 0.0015693727182224393.\n",
            "Epoch 4780: loss is 0.0015561076579615474.\n",
            "Epoch 4790: loss is 0.0015429669292643666.\n",
            "Epoch 4800: loss is 0.0015298888320103288.\n",
            "Epoch 4810: loss is 0.0015169490361586213.\n",
            "Epoch 4820: loss is 0.0015041076112538576.\n",
            "Epoch 4830: loss is 0.0014913666527718306.\n",
            "Epoch 4840: loss is 0.0014787581749260426.\n",
            "Epoch 4850: loss is 0.0014662165194749832.\n",
            "Epoch 4860: loss is 0.0014538030372932553.\n",
            "Epoch 4870: loss is 0.0014414838515222073.\n",
            "Epoch 4880: loss is 0.0014292763080447912.\n",
            "Epoch 4890: loss is 0.0014171809889376163.\n",
            "Epoch 4900: loss is 0.0014051418984308839.\n",
            "Epoch 4910: loss is 0.0013932279543951154.\n",
            "Epoch 4920: loss is 0.0013814166886731982.\n",
            "Epoch 4930: loss is 0.0013696973910555243.\n",
            "Epoch 4940: loss is 0.0013580991653725505.\n",
            "Epoch 4950: loss is 0.0013465668307617307.\n",
            "Epoch 4960: loss is 0.0013351368252187967.\n",
            "Epoch 4970: loss is 0.001323810196481645.\n",
            "Epoch 4980: loss is 0.0013125770492479205.\n",
            "Epoch 4990: loss is 0.0013014337746426463.\n",
            "Epoch 5000: loss is 0.001290396903641522.\n",
            "Epoch 5010: loss is 0.001279450603760779.\n",
            "Epoch 5020: loss is 0.0012685685651376843.\n",
            "Epoch 5030: loss is 0.001257803407497704.\n",
            "Epoch 5040: loss is 0.0012471352238208055.\n",
            "Epoch 5050: loss is 0.001236537005752325.\n",
            "Epoch 5060: loss is 0.001226044725626707.\n",
            "Epoch 5070: loss is 0.0012156360317021608.\n",
            "Epoch 5080: loss is 0.001205306383781135.\n",
            "Epoch 5090: loss is 0.001195077900774777.\n",
            "Epoch 5100: loss is 0.0011849276488646865.\n",
            "Epoch 5110: loss is 0.001174868899397552.\n",
            "Epoch 5120: loss is 0.0011648894287645817.\n",
            "Epoch 5130: loss is 0.001155020552687347.\n",
            "Epoch 5140: loss is 0.0011452024336904287.\n",
            "Epoch 5150: loss is 0.0011354655725881457.\n",
            "Epoch 5160: loss is 0.0011258164886385202.\n",
            "Epoch 5170: loss is 0.0011162672890350223.\n",
            "Epoch 5180: loss is 0.0011068072635680437.\n",
            "Epoch 5190: loss is 0.0010974039323627949.\n",
            "Epoch 5200: loss is 0.0010880859335884452.\n",
            "Epoch 5210: loss is 0.001078857691027224.\n",
            "Epoch 5220: loss is 0.001069699297659099.\n",
            "Epoch 5230: loss is 0.001060614362359047.\n",
            "Epoch 5240: loss is 0.0010516190668568015.\n",
            "Epoch 5250: loss is 0.0010426974622532725.\n",
            "Epoch 5260: loss is 0.0010338459396734834.\n",
            "Epoch 5270: loss is 0.0010250807972624898.\n",
            "Epoch 5280: loss is 0.0010163872502744198.\n",
            "Epoch 5290: loss is 0.0010077734477818012.\n",
            "Epoch 5300: loss is 0.000999222625978291.\n",
            "Epoch 5310: loss is 0.000990751781500876.\n",
            "Epoch 5320: loss is 0.000982356257736683.\n",
            "Epoch 5330: loss is 0.0009740315726958215.\n",
            "Epoch 5340: loss is 0.0009657736518420279.\n",
            "Epoch 5350: loss is 0.0009575911681167781.\n",
            "Epoch 5360: loss is 0.0009494967525824904.\n",
            "Epoch 5370: loss is 0.0009414478554390371.\n",
            "Epoch 5380: loss is 0.0009334733476862311.\n",
            "Epoch 5390: loss is 0.0009255760232917964.\n",
            "Epoch 5400: loss is 0.000917761935852468.\n",
            "Epoch 5410: loss is 0.0009099906892515719.\n",
            "Epoch 5420: loss is 0.0009022903977893293.\n",
            "Epoch 5430: loss is 0.0008946433663368225.\n",
            "Epoch 5440: loss is 0.0008870834717527032.\n",
            "Epoch 5450: loss is 0.0008795920293778181.\n",
            "Epoch 5460: loss is 0.0008721507037989795.\n",
            "Epoch 5470: loss is 0.0008647839422337711.\n",
            "Epoch 5480: loss is 0.0008574806270189583.\n",
            "Epoch 5490: loss is 0.000850251701194793.\n",
            "Epoch 5500: loss is 0.0008430614252574742.\n",
            "Epoch 5510: loss is 0.0008359472849406302.\n",
            "Epoch 5520: loss is 0.000828891817945987.\n",
            "Epoch 5530: loss is 0.0008219193550758064.\n",
            "Epoch 5540: loss is 0.0008149778586812317.\n",
            "Epoch 5550: loss is 0.0008081037667579949.\n",
            "Epoch 5560: loss is 0.0008012897451408207.\n",
            "Epoch 5570: loss is 0.000794539344497025.\n",
            "Epoch 5580: loss is 0.0007878603646531701.\n",
            "Epoch 5590: loss is 0.0007812217227183282.\n",
            "Epoch 5600: loss is 0.0007746399496681988.\n",
            "Epoch 5610: loss is 0.0007681240094825625.\n",
            "Epoch 5620: loss is 0.0007616773946210742.\n",
            "Epoch 5630: loss is 0.0007552637252956629.\n",
            "Epoch 5640: loss is 0.0007489169365726411.\n",
            "Epoch 5650: loss is 0.0007426172960549593.\n",
            "Epoch 5660: loss is 0.0007363928016275167.\n",
            "Epoch 5670: loss is 0.0007302063168026507.\n",
            "Epoch 5680: loss is 0.0007240778068080544.\n",
            "Epoch 5690: loss is 0.0007180006941780448.\n",
            "Epoch 5700: loss is 0.0007120036170817912.\n",
            "Epoch 5710: loss is 0.000706018996424973.\n",
            "Epoch 5720: loss is 0.000700095493812114.\n",
            "Epoch 5730: loss is 0.0006942338077351451.\n",
            "Epoch 5740: loss is 0.0006884172325953841.\n",
            "Epoch 5750: loss is 0.0006826700991950929.\n",
            "Epoch 5760: loss is 0.0006769491010345519.\n",
            "Epoch 5770: loss is 0.0006712874746881425.\n",
            "Epoch 5780: loss is 0.0006656767800450325.\n",
            "Epoch 5790: loss is 0.0006601173663511872.\n",
            "Epoch 5800: loss is 0.0006546018994413316.\n",
            "Epoch 5810: loss is 0.0006491360836662352.\n",
            "Epoch 5820: loss is 0.0006437180563807487.\n",
            "Epoch 5830: loss is 0.000638351368252188.\n",
            "Epoch 5840: loss is 0.0006330320029519498.\n",
            "Epoch 5850: loss is 0.0006277499487623572.\n",
            "Epoch 5860: loss is 0.0006225278484635055.\n",
            "Epoch 5870: loss is 0.0006173423607833683.\n",
            "Epoch 5880: loss is 0.0006122023332864046.\n",
            "Epoch 5890: loss is 0.0006071150419302285.\n",
            "Epoch 5900: loss is 0.0006020705914124846.\n",
            "Epoch 5910: loss is 0.0005970642087049782.\n",
            "Epoch 5920: loss is 0.0005920992116443813.\n",
            "Epoch 5930: loss is 0.0005871851462870836.\n",
            "Epoch 5940: loss is 0.0005823162500746548.\n",
            "Epoch 5950: loss is 0.000577499158680439.\n",
            "Epoch 5960: loss is 0.000572705757804215.\n",
            "Epoch 5970: loss is 0.0005679608439095318.\n",
            "Epoch 5980: loss is 0.00056325743207708.\n",
            "Epoch 5990: loss is 0.0005586068728007376.\n",
            "Epoch 6000: loss is 0.0005539684207178652.\n",
            "Epoch 6010: loss is 0.000549393065739423.\n",
            "Epoch 6020: loss is 0.0005448528681881726.\n",
            "Epoch 6030: loss is 0.0005403522518463433.\n",
            "Epoch 6040: loss is 0.0005358890048228204.\n",
            "Epoch 6050: loss is 0.0005314641166478395.\n",
            "Epoch 6060: loss is 0.0005270822439342737.\n",
            "Epoch 6070: loss is 0.0005227307556197047.\n",
            "Epoch 6080: loss is 0.0005184345063753426.\n",
            "Epoch 6090: loss is 0.0005141606088727713.\n",
            "Epoch 6100: loss is 0.0005099273985251784.\n",
            "Epoch 6110: loss is 0.0005057349917478859.\n",
            "Epoch 6120: loss is 0.0005015850765630603.\n",
            "Epoch 6130: loss is 0.0004974656039848924.\n",
            "Epoch 6140: loss is 0.0004933810560032725.\n",
            "Epoch 6150: loss is 0.0004893208970315754.\n",
            "Epoch 6160: loss is 0.00048531321226619184.\n",
            "Epoch 6170: loss is 0.0004813363775610924.\n",
            "Epoch 6180: loss is 0.00047739141155034304.\n",
            "Epoch 6190: loss is 0.0004734759568236768.\n",
            "Epoch 6200: loss is 0.00046960654435679317.\n",
            "Epoch 6210: loss is 0.0004657669342122972.\n",
            "Epoch 6220: loss is 0.0004619605897460133.\n",
            "Epoch 6230: loss is 0.00045819487422704697.\n",
            "Epoch 6240: loss is 0.0004544478142634034.\n",
            "Epoch 6250: loss is 0.0004507373960223049.\n",
            "Epoch 6260: loss is 0.0004470627463888377.\n",
            "Epoch 6270: loss is 0.00044341571629047394.\n",
            "Epoch 6280: loss is 0.00043980719055980444.\n",
            "Epoch 6290: loss is 0.0004362167092040181.\n",
            "Epoch 6300: loss is 0.00043267104774713516.\n",
            "Epoch 6310: loss is 0.0004291548393666744.\n",
            "Epoch 6320: loss is 0.00042566662887111306.\n",
            "Epoch 6330: loss is 0.0004222108400426805.\n",
            "Epoch 6340: loss is 0.00041878788033500314.\n",
            "Epoch 6350: loss is 0.00041538444929756224.\n",
            "Epoch 6360: loss is 0.0004120243538636714.\n",
            "Epoch 6370: loss is 0.0004086783737875521.\n",
            "Epoch 6380: loss is 0.00040537246968597174.\n",
            "Epoch 6390: loss is 0.00040208996506407857.\n",
            "Epoch 6400: loss is 0.0003988358366768807.\n",
            "Epoch 6410: loss is 0.0003956098516937345.\n",
            "Epoch 6420: loss is 0.000392413348890841.\n",
            "Epoch 6430: loss is 0.000389245426049456.\n",
            "Epoch 6440: loss is 0.00038610852789133787.\n",
            "Epoch 6450: loss is 0.0003829957277048379.\n",
            "Epoch 6460: loss is 0.00037991191493347287.\n",
            "Epoch 6470: loss is 0.0003768511232919991.\n",
            "Epoch 6480: loss is 0.00037382307345978916.\n",
            "Epoch 6490: loss is 0.0003708114963956177.\n",
            "Epoch 6500: loss is 0.00036783693940378726.\n",
            "Epoch 6510: loss is 0.0003648787969723344.\n",
            "Epoch 6520: loss is 0.00036194725544191897.\n",
            "Epoch 6530: loss is 0.0003590450796764344.\n",
            "Epoch 6540: loss is 0.0003561661287676543.\n",
            "Epoch 6550: loss is 0.0003533104027155787.\n",
            "Epoch 6560: loss is 0.00035048156860284507.\n",
            "Epoch 6570: loss is 0.0003476841957308352.\n",
            "Epoch 6580: loss is 0.00034489776589907706.\n",
            "Epoch 6590: loss is 0.00034213592880405486.\n",
            "Epoch 6600: loss is 0.00033941102446988225.\n",
            "Epoch 6610: loss is 0.0003366987220942974.\n",
            "Epoch 6620: loss is 0.00033400653046555817.\n",
            "Epoch 6630: loss is 0.0003313435008749366.\n",
            "Epoch 6640: loss is 0.00032870887662284076.\n",
            "Epoch 6650: loss is 0.0003260947996750474.\n",
            "Epoch 6660: loss is 0.000323490850860253.\n",
            "Epoch 6670: loss is 0.0003209240094292909.\n",
            "Epoch 6680: loss is 0.0003183762019034475.\n",
            "Epoch 6690: loss is 0.0003158440231345594.\n",
            "Epoch 6700: loss is 0.0003133390855509788.\n",
            "Epoch 6710: loss is 0.00031085408409126103.\n",
            "Epoch 6720: loss is 0.00030839190003462136.\n",
            "Epoch 6730: loss is 0.0003059484879486263.\n",
            "Epoch 6740: loss is 0.00030352550675161183.\n",
            "Epoch 6750: loss is 0.00030111969681456685.\n",
            "Epoch 6760: loss is 0.0002987434563692659.\n",
            "Epoch 6770: loss is 0.00029638316482305527.\n",
            "Epoch 6780: loss is 0.0002940393751487136.\n",
            "Epoch 6790: loss is 0.00029171965434215963.\n",
            "Epoch 6800: loss is 0.0002894217032007873.\n",
            "Epoch 6810: loss is 0.00028713958454318345.\n",
            "Epoch 6820: loss is 0.00028488136013038456.\n",
            "Epoch 6830: loss is 0.0002826374548021704.\n",
            "Epoch 6840: loss is 0.00028040900360792875.\n",
            "Epoch 6850: loss is 0.00027821175171993673.\n",
            "Epoch 6860: loss is 0.0002760241331998259.\n",
            "Epoch 6870: loss is 0.0002738621842581779.\n",
            "Epoch 6880: loss is 0.00027171801775693893.\n",
            "Epoch 6890: loss is 0.00026958243688568473.\n",
            "Epoch 6900: loss is 0.00026746917865239084.\n",
            "Epoch 6910: loss is 0.0002653759147506207.\n",
            "Epoch 6920: loss is 0.00026330031687393785.\n",
            "Epoch 6930: loss is 0.0002612425305414945.\n",
            "Epoch 6940: loss is 0.0002591967349871993.\n",
            "Epoch 6950: loss is 0.0002571757650002837.\n",
            "Epoch 6960: loss is 0.0002551673096604645.\n",
            "Epoch 6970: loss is 0.0002531776262912899.\n",
            "Epoch 6980: loss is 0.0002512055216357112.\n",
            "Epoch 6990: loss is 0.0002492497442290187.\n",
            "Epoch 7000: loss is 0.00024731401936151087.\n",
            "Epoch 7010: loss is 0.0002453846682328731.\n",
            "Epoch 7020: loss is 0.0002434809721307829.\n",
            "Epoch 7030: loss is 0.00024159067834261805.\n",
            "Epoch 7040: loss is 0.00023971761402208358.\n",
            "Epoch 7050: loss is 0.00023785688972566277.\n",
            "Epoch 7060: loss is 0.00023600873828399926.\n",
            "Epoch 7070: loss is 0.00023418049386236817.\n",
            "Epoch 7080: loss is 0.00023236479319166392.\n",
            "Epoch 7090: loss is 0.00023056886857375503.\n",
            "Epoch 7100: loss is 0.00022877994342707098.\n",
            "Epoch 7110: loss is 0.0002270160330226645.\n",
            "Epoch 7120: loss is 0.00022526600514538586.\n",
            "Epoch 7130: loss is 0.00022352463565766811.\n",
            "Epoch 7140: loss is 0.00022179998632054776.\n",
            "Epoch 7150: loss is 0.00022009234817232937.\n",
            "Epoch 7160: loss is 0.00021839869441464543.\n",
            "Epoch 7170: loss is 0.00021672018920071423.\n",
            "Epoch 7180: loss is 0.00021505127369891852.\n",
            "Epoch 7190: loss is 0.00021339747763704509.\n",
            "Epoch 7200: loss is 0.0002117551921401173.\n",
            "Epoch 7210: loss is 0.0002101282007060945.\n",
            "Epoch 7220: loss is 0.00020851800218224525.\n",
            "Epoch 7230: loss is 0.00020692752150353044.\n",
            "Epoch 7240: loss is 0.00020533590577542782.\n",
            "Epoch 7250: loss is 0.0002037628582911566.\n",
            "Epoch 7260: loss is 0.0002022076369030401.\n",
            "Epoch 7270: loss is 0.0002006605063797906.\n",
            "Epoch 7280: loss is 0.00019912628340534866.\n",
            "Epoch 7290: loss is 0.00019760655413847417.\n",
            "Epoch 7300: loss is 0.00019609108858276159.\n",
            "Epoch 7310: loss is 0.00019459707255009562.\n",
            "Epoch 7320: loss is 0.0001931145234266296.\n",
            "Epoch 7330: loss is 0.0001916426990646869.\n",
            "Epoch 7340: loss is 0.00019018584862351418.\n",
            "Epoch 7350: loss is 0.00018874401575885713.\n",
            "Epoch 7360: loss is 0.0001873104483820498.\n",
            "Epoch 7370: loss is 0.00018588904640637338.\n",
            "Epoch 7380: loss is 0.00018447583715897053.\n",
            "Epoch 7390: loss is 0.00018307393474970013.\n",
            "Epoch 7400: loss is 0.00018168517271988094.\n",
            "Epoch 7410: loss is 0.0001803137274691835.\n",
            "Epoch 7420: loss is 0.00017894426127895713.\n",
            "Epoch 7430: loss is 0.00017758940521162003.\n",
            "Epoch 7440: loss is 0.00017625185137148947.\n",
            "Epoch 7450: loss is 0.00017491572361905128.\n",
            "Epoch 7460: loss is 0.0001735953992465511.\n",
            "Epoch 7470: loss is 0.00017228668730240315.\n",
            "Epoch 7480: loss is 0.0001709864300210029.\n",
            "Epoch 7490: loss is 0.00016969555872492492.\n",
            "Epoch 7500: loss is 0.00016841573233250529.\n",
            "Epoch 7510: loss is 0.00016715453239157796.\n",
            "Epoch 7520: loss is 0.00016589174629189074.\n",
            "Epoch 7530: loss is 0.00016464492364320904.\n",
            "Epoch 7540: loss is 0.00016340914589818567.\n",
            "Epoch 7550: loss is 0.0001621770643396303.\n",
            "Epoch 7560: loss is 0.00016096243052743375.\n",
            "Epoch 7570: loss is 0.00015975296264514327.\n",
            "Epoch 7580: loss is 0.0001585544232511893.\n",
            "Epoch 7590: loss is 0.00015736457135062665.\n",
            "Epoch 7600: loss is 0.0001561862591188401.\n",
            "Epoch 7610: loss is 0.00015502001042477787.\n",
            "Epoch 7620: loss is 0.00015386258019134402.\n",
            "Epoch 7630: loss is 0.00015271507436409593.\n",
            "Epoch 7640: loss is 0.0001515712938271463.\n",
            "Epoch 7650: loss is 0.00015043935854919255.\n",
            "Epoch 7660: loss is 0.00014932412886992097.\n",
            "Epoch 7670: loss is 0.0001482059742556885.\n",
            "Epoch 7680: loss is 0.0001471044815843925.\n",
            "Epoch 7690: loss is 0.0001460059720557183.\n",
            "Epoch 7700: loss is 0.00014492061745841056.\n",
            "Epoch 7710: loss is 0.00014384553651325405.\n",
            "Epoch 7720: loss is 0.00014277463196776807.\n",
            "Epoch 7730: loss is 0.00014171276416163892.\n",
            "Epoch 7740: loss is 0.00014066303265281022.\n",
            "Epoch 7750: loss is 0.0001396199077134952.\n",
            "Epoch 7760: loss is 0.00013858739112038165.\n",
            "Epoch 7770: loss is 0.00013756171392742544.\n",
            "Epoch 7780: loss is 0.00013654619397129864.\n",
            "Epoch 7790: loss is 0.00013553340977523476.\n",
            "Epoch 7800: loss is 0.00013453229621518403.\n",
            "Epoch 7810: loss is 0.0001335361012024805.\n",
            "Epoch 7820: loss is 0.00013255173689685762.\n",
            "Epoch 7830: loss is 0.0001315764820901677.\n",
            "Epoch 7840: loss is 0.00013060001947451383.\n",
            "Epoch 7850: loss is 0.0001296403061132878.\n",
            "Epoch 7860: loss is 0.00012868743215221912.\n",
            "Epoch 7870: loss is 0.00012773969501722604.\n",
            "Epoch 7880: loss is 0.00012679969950113446.\n",
            "Epoch 7890: loss is 0.00012587437231559306.\n",
            "Epoch 7900: loss is 0.00012494200200308114.\n",
            "Epoch 7910: loss is 0.0001240256242454052.\n",
            "Epoch 7920: loss is 0.0001231201458722353.\n",
            "Epoch 7930: loss is 0.00012221705401316285.\n",
            "Epoch 7940: loss is 0.00012132126721553504.\n",
            "Epoch 7950: loss is 0.00012043231981806457.\n",
            "Epoch 7960: loss is 0.00011955027730436996.\n",
            "Epoch 7970: loss is 0.0001186785739264451.\n",
            "Epoch 7980: loss is 0.00011780828208429739.\n",
            "Epoch 7990: loss is 0.00011694490240188316.\n",
            "Epoch 8000: loss is 0.00011608644854277372.\n",
            "Epoch 8010: loss is 0.000115241389721632.\n",
            "Epoch 8020: loss is 0.00011439703666837886.\n",
            "Epoch 8030: loss is 0.0001135674465331249.\n",
            "Epoch 8040: loss is 0.00011274070857325569.\n",
            "Epoch 8050: loss is 0.0001119192093028687.\n",
            "Epoch 8060: loss is 0.00011110206105513498.\n",
            "Epoch 8070: loss is 0.00011029686720576137.\n",
            "Epoch 8080: loss is 0.00010949365969281644.\n",
            "Epoch 8090: loss is 0.00010870068217627704.\n",
            "Epoch 8100: loss is 0.00010791114618768916.\n",
            "Epoch 8110: loss is 0.00010712956282077357.\n",
            "Epoch 8120: loss is 0.00010635792568791658.\n",
            "Epoch 8130: loss is 0.00010558322537690401.\n",
            "Epoch 8140: loss is 0.00010481560457265005.\n",
            "Epoch 8150: loss is 0.00010405482316855341.\n",
            "Epoch 8160: loss is 0.00010330452641937882.\n",
            "Epoch 8170: loss is 0.00010255682718707249.\n",
            "Epoch 8180: loss is 0.00010181179095525295.\n",
            "Epoch 8190: loss is 0.00010107664274983108.\n",
            "Epoch 8200: loss is 0.00010035069135483354.\n",
            "Epoch 8210: loss is 9.962630429072306e-05.\n",
            "Epoch 8220: loss is 9.890684304991737e-05.\n",
            "Epoch 8230: loss is 9.819430852076039e-05.\n",
            "Epoch 8240: loss is 9.748985030455515e-05.\n",
            "Epoch 8250: loss is 9.678611968411133e-05.\n",
            "Epoch 8260: loss is 9.609234984964132e-05.\n",
            "Epoch 8270: loss is 9.540052997181192e-05.\n",
            "Epoch 8280: loss is 9.471869998378679e-05.\n",
            "Epoch 8290: loss is 9.404075535712764e-05.\n",
            "Epoch 8300: loss is 9.336571383755654e-05.\n",
            "Epoch 8310: loss is 9.269164002034813e-05.\n",
            "Epoch 8320: loss is 9.202871297020465e-05.\n",
            "Epoch 8330: loss is 9.137035522144288e-05.\n",
            "Epoch 8340: loss is 9.071699605556205e-05.\n",
            "Epoch 8350: loss is 9.006560867419466e-05.\n",
            "Epoch 8360: loss is 8.941810665419325e-05.\n",
            "Epoch 8370: loss is 8.87816131580621e-05.\n",
            "Epoch 8380: loss is 8.814807370072231e-05.\n",
            "Epoch 8390: loss is 8.75179102877155e-05.\n",
            "Epoch 8400: loss is 8.689371315995231e-05.\n",
            "Epoch 8410: loss is 8.627099305158481e-05.\n",
            "Epoch 8420: loss is 8.565418829675764e-05.\n",
            "Epoch 8430: loss is 8.504701690981165e-05.\n",
            "Epoch 8440: loss is 8.444020932074636e-05.\n",
            "Epoch 8450: loss is 8.383927342947572e-05.\n",
            "Epoch 8460: loss is 8.32422447274439e-05.\n",
            "Epoch 8470: loss is 8.265324140666053e-05.\n",
            "Epoch 8480: loss is 8.206602797145024e-05.\n",
            "Epoch 8490: loss is 8.148573397193104e-05.\n",
            "Epoch 8500: loss is 8.090642950264737e-05.\n",
            "Epoch 8510: loss is 8.033285121200606e-05.\n",
            "Epoch 8520: loss is 7.976253255037591e-05.\n",
            "Epoch 8530: loss is 7.919910422060639e-05.\n",
            "Epoch 8540: loss is 7.86366144893691e-05.\n",
            "Epoch 8550: loss is 7.808023656252772e-05.\n",
            "Epoch 8560: loss is 7.752855162834749e-05.\n",
            "Epoch 8570: loss is 7.698081753915176e-05.\n",
            "Epoch 8580: loss is 7.643607386853546e-05.\n",
            "Epoch 8590: loss is 7.589420420117676e-05.\n",
            "Epoch 8600: loss is 7.53584026824683e-05.\n",
            "Epoch 8610: loss is 7.482254295609891e-05.\n",
            "Epoch 8620: loss is 7.430090045090765e-05.\n",
            "Epoch 8630: loss is 7.377559813903645e-05.\n",
            "Epoch 8640: loss is 7.325855403905734e-05.\n",
            "Epoch 8650: loss is 7.274347444763407e-05.\n",
            "Epoch 8660: loss is 7.223016291391104e-05.\n",
            "Epoch 8670: loss is 7.172504410846159e-05.\n",
            "Epoch 8680: loss is 7.12198598193936e-05.\n",
            "Epoch 8690: loss is 7.07206127117388e-05.\n",
            "Epoch 8700: loss is 7.02243996784091e-05.\n",
            "Epoch 8710: loss is 6.973002746235579e-05.\n",
            "Epoch 8720: loss is 6.92416142555885e-05.\n",
            "Epoch 8730: loss is 6.875713734189048e-05.\n",
            "Epoch 8740: loss is 6.827466131653637e-05.\n",
            "Epoch 8750: loss is 6.779411341995001e-05.\n",
            "Epoch 8760: loss is 6.732150359312072e-05.\n",
            "Epoch 8770: loss is 6.684988329652697e-05.\n",
            "Epoch 8780: loss is 6.638315244344994e-05.\n",
            "Epoch 8790: loss is 6.591749843209982e-05.\n",
            "Epoch 8800: loss is 6.546075019286945e-05.\n",
            "Epoch 8810: loss is 6.499900337075815e-05.\n",
            "Epoch 8820: loss is 6.454528192989528e-05.\n",
            "Epoch 8830: loss is 6.409834895748645e-05.\n",
            "Epoch 8840: loss is 6.365240551531315e-05.\n",
            "Epoch 8850: loss is 6.320930697256699e-05.\n",
            "Epoch 8860: loss is 6.276837666518986e-05.\n",
            "Epoch 8870: loss is 6.233140447875485e-05.\n",
            "Epoch 8880: loss is 6.1893391830381e-05.\n",
            "Epoch 8890: loss is 6.146636587800458e-05.\n",
            "Epoch 8900: loss is 6.103923078626394e-05.\n",
            "Epoch 8910: loss is 6.0616050177486613e-05.\n",
            "Epoch 8920: loss is 6.019582724547945e-05.\n",
            "Epoch 8930: loss is 5.9781570598715916e-05.\n",
            "Epoch 8940: loss is 5.936727757216431e-05.\n",
            "Epoch 8950: loss is 5.8953977713827044e-05.\n",
            "Epoch 8960: loss is 5.8548608649289235e-05.\n",
            "Epoch 8970: loss is 5.814532414660789e-05.\n",
            "Epoch 8980: loss is 5.773792145191692e-05.\n",
            "Epoch 8990: loss is 5.7337463658768684e-05.\n",
            "Epoch 9000: loss is 5.693998173228465e-05.\n",
            "Epoch 9010: loss is 5.654642154695466e-05.\n",
            "Epoch 9020: loss is 5.6158874940592796e-05.\n",
            "Epoch 9030: loss is 5.577032425208017e-05.\n",
            "Epoch 9040: loss is 5.538592449738644e-05.\n",
            "Epoch 9050: loss is 5.500412953551859e-05.\n",
            "Epoch 9060: loss is 5.4625470511382446e-05.\n",
            "Epoch 9070: loss is 5.424977280199528e-05.\n",
            "Epoch 9080: loss is 5.387810597312637e-05.\n",
            "Epoch 9090: loss is 5.3505315008806065e-05.\n",
            "Epoch 9100: loss is 5.313949804985896e-05.\n",
            "Epoch 9110: loss is 5.277070522424765e-05.\n",
            "Epoch 9120: loss is 5.2408937335712835e-05.\n",
            "Epoch 9130: loss is 5.20500325364992e-05.\n",
            "Epoch 9140: loss is 5.169013820705004e-05.\n",
            "Epoch 9150: loss is 5.1337188779143617e-05.\n",
            "Epoch 9160: loss is 5.0984210247406736e-05.\n",
            "Epoch 9170: loss is 5.0635258958209306e-05.\n",
            "Epoch 9180: loss is 5.028627856518142e-05.\n",
            "Epoch 9190: loss is 4.994025221094489e-05.\n",
            "Epoch 9200: loss is 4.9600239435676485e-05.\n",
            "Epoch 9210: loss is 4.9258134822594e-05.\n",
            "Epoch 9220: loss is 4.8923000576905906e-05.\n",
            "Epoch 9230: loss is 4.8587786295684054e-05.\n",
            "Epoch 9240: loss is 4.8256646550726146e-05.\n",
            "Epoch 9250: loss is 4.792549952981062e-05.\n",
            "Epoch 9260: loss is 4.759929288411513e-05.\n",
            "Epoch 9270: loss is 4.727312261820771e-05.\n",
            "Epoch 9280: loss is 4.695384268416092e-05.\n",
            "Epoch 9290: loss is 4.663255822379142e-05.\n",
            "Epoch 9300: loss is 4.6315235522342846e-05.\n",
            "Epoch 9310: loss is 4.599789099302143e-05.\n",
            "Epoch 9320: loss is 4.568952135741711e-05.\n",
            "Epoch 9330: loss is 4.537716449704021e-05.\n",
            "Epoch 9340: loss is 4.506974801188335e-05.\n",
            "Epoch 9350: loss is 4.476428512134589e-05.\n",
            "Epoch 9360: loss is 4.4459851778810844e-05.\n",
            "Epoch 9370: loss is 4.415640069055371e-05.\n",
            "Epoch 9380: loss is 4.386101136333309e-05.\n",
            "Epoch 9390: loss is 4.355943383416161e-05.\n",
            "Epoch 9400: loss is 4.32629203714896e-05.\n",
            "Epoch 9410: loss is 4.297135092201643e-05.\n",
            "Epoch 9420: loss is 4.267981057637371e-05.\n",
            "Epoch 9430: loss is 4.2392162868054584e-05.\n",
            "Epoch 9440: loss is 4.21075128542725e-05.\n",
            "Epoch 9450: loss is 4.181691838311963e-05.\n",
            "Epoch 9460: loss is 4.1535207856213674e-05.\n",
            "Epoch 9470: loss is 4.125457053305581e-05.\n",
            "Epoch 9480: loss is 4.097489727428183e-05.\n",
            "Epoch 9490: loss is 4.0695202187635005e-05.\n",
            "Epoch 9500: loss is 4.042045839014463e-05.\n",
            "Epoch 9510: loss is 4.0146733226720244e-05.\n",
            "Epoch 9520: loss is 3.9878937968751416e-05.\n",
            "Epoch 9530: loss is 3.961013862863183e-05.\n",
            "Epoch 9540: loss is 3.9344289689324796e-05.\n",
            "Epoch 9550: loss is 3.907846985384822e-05.\n",
            "Epoch 9560: loss is 3.8814636354800314e-05.\n",
            "Epoch 9570: loss is 3.855372779071331e-05.\n",
            "Epoch 9580: loss is 3.8293870602501556e-05.\n",
            "Epoch 9590: loss is 3.803698200499639e-05.\n",
            "Epoch 9600: loss is 3.77800824935548e-05.\n",
            "Epoch 9610: loss is 3.7525180232478306e-05.\n",
            "Epoch 9620: loss is 3.727121293195523e-05.\n",
            "Epoch 9630: loss is 3.7021229218225926e-05.\n",
            "Epoch 9640: loss is 3.677422500913963e-05.\n",
            "Epoch 9650: loss is 3.652925443020649e-05.\n",
            "Epoch 9660: loss is 3.6286201066104695e-05.\n",
            "Epoch 9670: loss is 3.604016819735989e-05.\n",
            "Epoch 9680: loss is 3.5797122109215707e-05.\n",
            "Epoch 9690: loss is 3.555803414201364e-05.\n",
            "Epoch 9700: loss is 3.5321958421263844e-05.\n",
            "Epoch 9710: loss is 3.508386362227611e-05.\n",
            "Epoch 9720: loss is 3.4849705116357654e-05.\n",
            "Epoch 9730: loss is 3.4615586628206074e-05.\n",
            "Epoch 9740: loss is 3.438545172684826e-05.\n",
            "Epoch 9750: loss is 3.4157281334046274e-05.\n",
            "Epoch 9760: loss is 3.392813232494518e-05.\n",
            "Epoch 9770: loss is 3.370091872056946e-05.\n",
            "Epoch 9780: loss is 3.347669553477317e-05.\n",
            "Epoch 9790: loss is 3.3253469155170023e-05.\n",
            "Epoch 9800: loss is 3.302826007711701e-05.\n",
            "Epoch 9810: loss is 3.2808995456434786e-05.\n",
            "Epoch 9820: loss is 3.2590727641945705e-05.\n",
            "Epoch 9830: loss is 3.2383552024839446e-05.\n",
            "Epoch 9840: loss is 3.217928679077886e-05.\n",
            "Epoch 9850: loss is 3.1972969736671075e-05.\n",
            "Epoch 9860: loss is 3.175464007654227e-05.\n",
            "Epoch 9870: loss is 3.152817225782201e-05.\n",
            "Epoch 9880: loss is 3.131872654194012e-05.\n",
            "Epoch 9890: loss is 3.111043042736128e-05.\n",
            "Epoch 9900: loss is 3.090210520895198e-05.\n",
            "Epoch 9910: loss is 3.0695686291437596e-05.\n",
            "Epoch 9920: loss is 3.0492316000163555e-05.\n",
            "Epoch 9930: loss is 3.028891296708025e-05.\n",
            "Epoch 9940: loss is 3.009043757629115e-05.\n",
            "Epoch 9950: loss is 2.989504355355166e-05.\n",
            "Epoch 9960: loss is 2.9693574106204323e-05.\n",
            "Epoch 9970: loss is 2.949804184027016e-05.\n",
            "Epoch 9980: loss is 2.9298604204086587e-05.\n",
            "Epoch 9990: loss is 2.9106127840350382e-05.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(range(epochs), losses)\n",
        "plt.xlabel('epoch')\n",
        "plt.ylabel('loss')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 449
        },
        "id": "gmZa3bb-TdR9",
        "outputId": "97eab338-2a81-4da7-a6df-425cc56f8256"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate on test set\n",
        "with torch.no_grad():\n",
        "  y_eval = model.forward(X_test)\n",
        "  loss = criterion(y_eval, y_test)"
      ],
      "metadata": {
        "id": "PoU5kh4STxgk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uABpqS_CUTYr",
        "outputId": "741461af-a219-411a-97d2-79ecc25ec817"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(0.9087)"
            ]
          },
          "metadata": {},
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.no_grad():\n",
        "  correct = 0\n",
        "  for i, data in enumerate(X_test):\n",
        "    y_val = model.forward(data)\n",
        "    print(f'{i+1}.) {str(y_val)}, {y_test[i]}')\n",
        "    if y_val.argmax().item() == y_test[i]:\n",
        "      correct += 1\n",
        "    else: print('Wrong here!')\n",
        "  print(correct)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ro_xreLFUT1p",
        "outputId": "89bf1898-6226-4aab-99aa-4806d303cfdd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.) tensor([-10.9115,  23.0720,  -7.2917]), 1\n",
            "2.) tensor([-24.0103,  74.9278, -44.5817]), 1\n",
            "3.) tensor([-27.5630,  87.1691, -52.5951]), 1\n",
            "4.) tensor([ -0.7538, -10.4100,  15.1408]), 2\n",
            "5.) tensor([-19.7296,  56.2723, -30.3649]), 1\n",
            "6.) tensor([  1.4248, -14.4732,  16.5479]), 2\n",
            "7.) tensor([-11.2324,  22.8591,  -6.4261]), 1\n",
            "8.) tensor([-1.1992, -9.7486, 15.0830]), 2\n",
            "9.) tensor([-15.3350,  38.6554, -17.5691]), 1\n",
            "10.) tensor([-26.4565,  83.3668, -50.0588]), 1\n",
            "11.) tensor([-8.8123, 12.3527,  1.6082]), 1\n",
            "12.) tensor([ 23.9534, -33.5865,   6.8763]), 0\n",
            "13.) tensor([ 21.6217, -30.2489,   6.0875]), 0\n",
            "14.) tensor([  2.8353, -15.5634,  15.3675]), 2\n",
            "15.) tensor([ 20.2195, -31.3681,   9.3834]), 0\n",
            "16.) tensor([-5.1915, -1.0154, 10.8766]), 1\n",
            "Wrong here!\n",
            "17.) tensor([ 21.8502, -31.3880,   7.1442]), 0\n",
            "18.) tensor([-9.2305, 14.8147, -0.5534]), 2\n",
            "Wrong here!\n",
            "19.) tensor([ 23.3887, -32.4180,   6.2508]), 0\n",
            "20.) tensor([ 18.8649, -27.7598,   6.9121]), 0\n",
            "21.) tensor([  2.6402, -15.8632,  16.1425]), 2\n",
            "22.) tensor([-24.2021,  74.0395, -43.1683]), 1\n",
            "23.) tensor([ 20.0577, -31.3695,   9.6097]), 0\n",
            "24.) tensor([ 23.0366, -31.9317,   6.1532]), 0\n",
            "25.) tensor([  2.0346, -15.1794,  16.3565]), 2\n",
            "26.) tensor([  2.1371, -15.2626,  16.2760]), 2\n",
            "27.) tensor([-1.5223, -9.3671, 15.1747]), 2\n",
            "28.) tensor([  2.5182, -16.2765,  16.9337]), 2\n",
            "29.) tensor([ 23.7630, -33.0988,   6.5524]), 0\n",
            "30.) tensor([-1.2752, -9.2984, 14.6151]), 2\n",
            "28\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Predict new data\n",
        "import numpy as np\n",
        "new_data = np.random.rand(1,4)\n",
        "new_data = torch.FloatTensor(new_data)\n",
        "with torch.no_grad():\n",
        "  print(model(new_data).argmax().item())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6nBYCh7dWGSc",
        "outputId": "7102d8b4-fc89-42a8-d2a6-00506098c0aa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## CNN"
      ],
      "metadata": {
        "id": "qdvjyHzXKxZy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms\n",
        "from torchvision.utils import make_grid\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "iFoUeJ04Ky6S"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transform = transforms.ToTensor() # images to number_of_images, Height, Weight, Channel\n",
        "# Train Data\n",
        "train_data = datasets.MNIST(root ='cnn_data', train = True, download = True, transform = transform)"
      ],
      "metadata": {
        "id": "-Qn4ke41LhEe"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_data = datasets.MNIST(root ='cnn_data', train = False, download = True, transform = transforms)"
      ],
      "metadata": {
        "id": "kvKaTvI-MNJ6"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KdRM0WJGMOsc",
        "outputId": "93de53b5-20aa-4bd8-c3bd-3fe32becef4d"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Dataset MNIST\n",
              "    Number of datapoints: 10000\n",
              "    Root location: cnn_data\n",
              "    Split: Test\n",
              "    StandardTransform\n",
              "Transform: <module 'torchvision.transforms' from '/usr/local/lib/python3.11/dist-packages/torchvision/transforms/__init__.py'>"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yaJLGnQMMlLs",
        "outputId": "f7484e65-bd2c-43ac-b31a-a55444bcf639"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Dataset MNIST\n",
              "    Number of datapoints: 60000\n",
              "    Root location: cnn_data\n",
              "    Split: Train\n",
              "    StandardTransform\n",
              "Transform: <module 'torchvision.transforms' from '/usr/local/lib/python3.11/dist-packages/torchvision/transforms/__init__.py'>"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader = DataLoader(train_data, batch_size = 10)\n",
        "test_loader = DataLoader(test_data, batch_size = 10)\n"
      ],
      "metadata": {
        "id": "nQXhbon8S95R"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "iter(train_loader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cbv_q1RbU-gB",
        "outputId": "ea49fa2c-86bf-42de-9cd4-3a8459e14c1e"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch.utils.data.dataloader._SingleProcessDataLoaderIter at 0x7fcfe5b490d0>"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# View one batch of images and labels\n",
        "images, labels = next(iter(train_loader))\n",
        "\n",
        "# Check shape\n",
        "print(\"Image batch shape:\", images.shape)  # e.g., torch.Size([10, 1, 28, 28])\n",
        "print(\"Label batch shape:\", labels.shape)\n",
        "\n",
        "# Show the first image in the batch\n",
        "plt.imshow(images[0].squeeze(), cmap='gray')\n",
        "plt.title(f\"Label: {labels[0].item()}\")\n",
        "plt.axis('off')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 463
        },
        "id": "xqq_At_HTGxt",
        "outputId": "1c104587-c5f4-449f-c9fd-8cf1f5908b6e"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Image batch shape: torch.Size([10, 1, 28, 28])\n",
            "Label batch shape: torch.Size([10])\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Model\n",
        "class CNN(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.conv1 = nn.Conv2d(1,6,3,1)\n",
        "    self.conv2 = nn.Conv2d(6,16,3,1)\n",
        "    self.fc1 = nn.Linear(5*5*16, 120)\n",
        "    self.fc2 = nn.Linear(120, 60)\n",
        "    self.fc3 = nn.Linear(60, 10)\n",
        "\n",
        "  def forward(self, X):\n",
        "    X = F.relu(self.conv1(X))\n",
        "    X = F.max_pool2d(X,2,2)\n",
        "    X = F.relu(self.conv2(X))\n",
        "    X = F.max_pool2d(X,2,2)\n",
        "    X = X.reshape(-1, 5*5*16) # Mean we get X.shape = (n_number data ,16 feature 5x5 for each data)\n",
        "    X = F.relu(self.fc1(X))\n",
        "    X = F.relu(self.fc2(X))\n",
        "    X = self.fc3(X)\n",
        "    return F.log_softmax(X, dim = 1)"
      ],
      "metadata": {
        "id": "GhHL9LFeMnZM"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(42)\n",
        "epochs = 2\n",
        "losses = []\n",
        "model = CNN()\n",
        "model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a9Ea5J7bRUxS",
        "outputId": "9a18b351-23e3-4ccd-f1d9-cd2a9ce53552"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CNN(\n",
              "  (conv1): Conv2d(1, 6, kernel_size=(3, 3), stride=(1, 1))\n",
              "  (conv2): Conv2d(6, 16, kernel_size=(3, 3), stride=(1, 1))\n",
              "  (fc1): Linear(in_features=400, out_features=120, bias=True)\n",
              "  (fc2): Linear(in_features=120, out_features=60, bias=True)\n",
              "  (fc3): Linear(in_features=60, out_features=10, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr = 0.001)"
      ],
      "metadata": {
        "id": "_RYWdy92RoFs"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "start_time = time.time()\n",
        "\n",
        "for i in range(epochs):\n",
        "\n",
        "  for b, (X_train, y_train) in enumerate(train_loader):\n",
        "    y_pred = model(X_train)\n",
        "    loss = criterion(y_pred, y_train)\n",
        "\n",
        "    losses.append(loss)\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    if b % 500 == 0:\n",
        "      print(loss)\n",
        "  print(i)\n",
        "end_time = time.time()\n",
        "total = end_time - start_time\n",
        "print(f\"Time consumed: {total/60} minutes.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IvplE6f7R3Nc",
        "outputId": "70c54bd7-3eac-4276-878a-3c2c8933ed04"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(2.3763, grad_fn=<NllLossBackward0>)\n",
            "tensor(0.6363, grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0711, grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0087, grad_fn=<NllLossBackward0>)\n",
            "tensor(0.3040, grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0211, grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0630, grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0111, grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0905, grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0123, grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0169, grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0044, grad_fn=<NllLossBackward0>)\n",
            "0\n",
            "tensor(0.0023, grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0044, grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0073, grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0018, grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0585, grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0078, grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0556, grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0013, grad_fn=<NllLossBackward0>)\n",
            "tensor(0.2101, grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0150, grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0144, grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0038, grad_fn=<NllLossBackward0>)\n",
            "1\n",
            "Time consumed: 1.3252738197644551 minutes.\n"
          ]
        }
      ]
    }
  ]
}